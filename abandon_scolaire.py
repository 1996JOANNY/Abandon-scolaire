# ============================================================================
# IMPORTS DES BIBLIOTH√àQUES
# ============================================================================

import streamlit as st                    # Interface web interactive pour cr√©er des applications de donn√©es
import pandas as pd                       # Manipulation et analyse de donn√©es structur√©es
import numpy as np                        # Calculs num√©riques et op√©rations sur les arrays
import seaborn as sns                     # Visualisations statistiques avanc√©es bas√©es sur matplotlib
import matplotlib.pyplot as plt          # Cr√©ation de graphiques et visualisations de base
import plotly.express as px              # Graphiques interactifs et modernes
from sklearn.cluster import KMeans       # Algorithme de clustering non supervis√©
from sklearn.ensemble import RandomForestClassifier  # Classificateur par for√™t al√©atoire
from sklearn.preprocessing import StandardScaler     # Normalisation des donn√©es
from mlxtend.frequent_patterns import apriori, association_rules  # Analyse des r√®gles d'association
import io                                # Gestion des flux d'entr√©e/sortie en m√©moire
from reportlab.pdfgen import canvas      # G√©n√©ration de documents PDF
from reportlab.lib.pagesizes import letter  # Formats de page pour PDF

# ============================================================================
# CONFIGURATION DE L'INTERFACE STREAMLIT
# ============================================================================

# Configuration de la page principale avec titre et mise en page large
st.set_page_config(page_title="üéì Pr√©vention de l'abandon scolaire", layout="wide")

# Injection de CSS personnalis√© pour styliser l'application
st.markdown("""
    <style>
        .main {background-color: #f9fafa;}          /* Couleur de fond gris clair */
        h1, h2, h3 {color: #004080;}                /* Titres en bleu fonc√© */
        .stButton>button {background-color: #007acc; color: white; border-radius: 8px;}  /* Boutons bleus arrondis */
    </style>
""", unsafe_allow_html=True)

# ============================================================================
# FONCTION DE CHARGEMENT ET PR√âPARATION DES DONN√âES OULAD
# ============================================================================

@st.cache_data  # Cache pour √©viter de recharger les donn√©es √† chaque interaction
def load_oulad_data():
    """
    Charge et pr√©process les donn√©es du dataset OULAD (Open University Learning Analytics Dataset)
    Fusionne plusieurs fichiers CSV pour cr√©er un dataset unifi√© d'analyse
    """
    # Chargement des trois fichiers CSV principaux du dataset OULAD
    assessments = pd.read_csv("assessments.csv")           # Informations sur les √©valuations
    student_assessment = pd.read_csv("studentAssessment.csv")  # R√©sultats des √©tudiants aux √©valuations
    student_info = pd.read_csv("studentInfo.csv")          # Informations d√©mographiques des √©tudiants
    
    # Fusion des donn√©es : d'abord √©valuations avec r√©sultats
    merged = student_assessment.merge(assessments, on='id_assessment', how='left')
    # Puis ajout des informations √©tudiants
    merged = merged.merge(student_info, on='id_student', how='left')
    
    # Agr√©gation des donn√©es par √©tudiant pour avoir une ligne par √©tudiant
    df_grouped = merged.groupby('id_student').agg({
        'score': 'mean',                    # Score moyen de l'√©tudiant
        'date_submitted': 'count',          # Nombre d'√©valuations soumises
        'studied_credits': 'mean',          # Cr√©dits moyens √©tudi√©s
        'age_band': 'first',               # Tranche d'√¢ge (premi√®re valeur)
        'gender': 'first',                 # Genre (premi√®re valeur)
        'region': 'first',                 # R√©gion (premi√®re valeur)
        'highest_education': 'first',      # Niveau d'√©ducation le plus √©lev√©
        'disability': 'first',             # Statut de handicap
        'final_result': 'first'            # R√©sultat final du cours
    }).reset_index()
    
    # Renommage des colonnes en fran√ßais pour plus de clart√©
    df_grouped.rename(columns={
        'score': 'score_moyen',
        'date_submitted': 'nb_evaluations',
        'studied_credits': 'credits_etudies',
        'age_band': 'age',
        'gender': 'sexe',
        'highest_education': 'niveau_parental',
        'disability': 'handicap',
        'final_result': 'abandon'
    }, inplace=True)
    
    # G√©n√©ration de donn√©es synth√©tiques pour enrichir l'analyse
    df_grouped['temps_moodle'] = np.random.uniform(0, 20, size=len(df_grouped))        # Temps pass√© sur Moodle (heures)
    df_grouped['participation_forum'] = np.random.randint(0, 50, size=len(df_grouped)) # Nombre de posts sur forum
    df_grouped['satisfaction'] = np.random.uniform(1, 5, size=len(df_grouped))         # Note de satisfaction (1-5)
    
    # Transformation de la variable cible : 1 si abandon ('Withdrawn' ou 'Fail'), 0 sinon
    df_grouped['abandon'] = df_grouped['abandon'].map(lambda x: 1 if x in ['Withdrawn', 'Fail'] else 0)
    
    return df_grouped

# ============================================================================
# FONCTION DE PR√âPROCESSING DES DONN√âES
# ============================================================================

def preprocess(df):
    """
    Pr√©process les donn√©es pour les pr√©parer √† l'analyse machine learning
    - G√®re les valeurs manquantes
    - Encode les variables cat√©gorielles
    """
    # Remplacement des valeurs manquantes par la moyenne pour les colonnes num√©riques
    df = df.fillna(df.mean(numeric_only=True))
    
    # Identification des colonnes cat√©gorielles (type object)
    cat_cols = df.select_dtypes(include='object').columns
    
    # Encodage one-hot des variables cat√©gorielles (drop_first=True √©vite la multicolin√©arit√©)
    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)
    
    return df

# ============================================================================
# PIPELINE PRINCIPAL D'ANALYSE
# ============================================================================

def analysis_pipeline(df, label, pdf_key):
    """
    Pipeline complet d'analyse des donn√©es incluant :
    - Visualisations exploratoires
    - Clustering
    - Classification
    - R√®gles d'association
    - Simulation individuelle
    """
    # Pr√©processing des donn√©es
    df = preprocess(df)

    # ========================================================================
    # SECTION 1 : VISUALISATIONS DE BASE
    # ========================================================================
    
    # Cr√©ation de deux colonnes pour l'affichage c√¥te √† c√¥te
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Histogramme")
        # Histogramme du score moyen color√© par statut d'abandon
        st.plotly_chart(px.histogram(df, x=label, color='abandon', 
                                   color_discrete_sequence=['#1f77b4', '#ff7f0e']))

    with col2:
        st.subheader("üî• Heatmap des corr√©lations")
        # Matrice de corr√©lation entre toutes les variables num√©riques
        fig, ax = plt.subplots(figsize=(6,4))
        sns.heatmap(df.corr(), cmap='viridis', ax=ax)
        st.pyplot(fig)

    # ========================================================================
    # SECTION 2 : ANALYSE D√âTAILL√âE AVEC BOXPLOTS
    # ========================================================================
    
    with st.expander("üì¶ Boxplots d√©taill√©s"):
        # Identification des colonnes num√©riques
        num_cols = df.select_dtypes(include=np.number).columns
        
        # Cr√©ation d'un boxplot pour chaque variable num√©rique (sauf 'abandon')
        for col in num_cols:
            if col != 'abandon':
                # Boxplot comparant les distributions entre √©tudiants qui abandonnent et ceux qui r√©ussissent
                fig = px.box(df, y=col, color='abandon', 
                           color_discrete_sequence=['#2ca02c', '#d62728'])
                st.plotly_chart(fig)

    # ========================================================================
    # SECTION 3 : CLUSTERING K-MEANS
    # ========================================================================
    
    st.subheader("üîç Clustering K-Means")
    
    # Standardisation des donn√©es pour le clustering
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df.drop(['abandon'], axis=1, errors='ignore'))
    
    # Application de K-Means avec 3 clusters
    df['cluster'] = KMeans(n_clusters=3, random_state=42).fit_predict(X_scaled)
    
    # Visualisation des clusters dans un scatter plot 2D
    st.plotly_chart(px.scatter(df, x=label, y='temps_moodle', color='cluster', 
                             color_continuous_scale='Turbo'))

    # ========================================================================
    # SECTION 4 : CLASSIFICATION RANDOM FOREST
    # ========================================================================
    
    with st.expander("ü§ñ Classification Random Forest"):
        # Pr√©paration des donn√©es pour la classification
        X = df.drop(['abandon', 'cluster'], axis=1, errors='ignore')  # Variables explicatives
        y = df['abandon']                                             # Variable cible
        
        # Entra√Ænement du mod√®le Random Forest
        clf = RandomForestClassifier().fit(X, y)
        st.success("Le mod√®le Random Forest a √©t√© entra√Æn√© avec succ√®s.")

    # ========================================================================
    # SECTION 5 : R√àGLES D'ASSOCIATION
    # ========================================================================
    
    with st.expander("üìà R√®gles d'association"):
        # Copie du dataframe pour la transformation
        df_assoc = df.copy()
        
        # Discr√©tisation des variables continues en 3 cat√©gories (Low, Medium, High)
        for col in df_assoc.select_dtypes(include=['float64', 'int64']).columns:
            if df_assoc[col].nunique() >= 3:  # Seulement si au moins 3 valeurs uniques
                df_assoc[col] = pd.qcut(df_assoc[col], q=3, labels=['Low', 'Medium', 'High'], 
                                      duplicates='drop')
        
        # Encodage one-hot pour l'algorithme Apriori
        df_assoc = pd.get_dummies(df_assoc)
        
        # Filtrage pour ne garder que les colonnes binaires (0 et 1)
        df_assoc = df_assoc.loc[:, df_assoc.apply(lambda x: set(x.unique()).issubset({0, 1}))]
        
        if not df_assoc.empty:
            # Application de l'algorithme Apriori pour trouver les itemsets fr√©quents
            freq_items = apriori(df_assoc, min_support=0.1, use_colnames=True)
            
            # G√©n√©ration des r√®gles d'association
            rules = association_rules(freq_items, metric='confidence', min_threshold=0.6)
            
            # Affichage des r√®gles les plus int√©ressantes
            st.dataframe(rules[['antecedents', 'consequents', 'support', 'confidence']])
        else:
            st.info("Aucune r√®gle d'association trouv√©e.")

    # ========================================================================
    # SECTION 6 : SIMULATION INDIVIDUELLE ET G√âN√âRATION DE RAPPORT
    # ========================================================================
    
    with st.expander("üéØ Simulation individuelle et rapport"):
        # Cr√©ation d'un formulaire interactif pour saisir les donn√©es d'un √©tudiant
        input_data = {}
        
        # Pour chaque variable du mod√®le, cr√©ation d'un widget d'entr√©e appropri√©
        for col in X.columns:
            if X[col].dtype in ['float64', 'int64']:
                # Slider pour les variables num√©riques
                input_data[col] = st.slider(f"{col}", 
                                          float(X[col].min()), 
                                          float(X[col].max()), 
                                          float(X[col].mean()))
            else:
                # Selectbox pour les variables cat√©gorielles
                input_data[col] = st.selectbox(f"{col}", X[col].unique())
        
        # Transformation des donn√©es d'entr√©e en DataFrame
        input_df = pd.DataFrame([input_data])
        
        # Alignement avec les colonnes du mod√®le (ajout de colonnes manquantes avec 0)
        input_df = pd.get_dummies(input_df).reindex(columns=X.columns, fill_value=0)
        
        # Pr√©diction du risque d'abandon (probabilit√© de la classe positive)
        risk = clf.predict_proba(input_df)[0][1]
        
        # Affichage du risque sous forme de m√©trique
        st.metric(label="Risque d'abandon estim√©", value=f"{risk:.2%}")

        # ====================================================================
        # G√âN√âRATION DE RAPPORT PDF
        # ====================================================================
        
        if st.button("üì• G√©n√©rer le rapport PDF", key=pdf_key):
            # Cr√©ation d'un buffer en m√©moire pour le PDF
            buffer = io.BytesIO()
            
            # Cr√©ation d'un canvas PDF avec format letter
            c = canvas.Canvas(buffer, pagesize=letter)
            
            # √âcriture du risque d'abandon en haut de la page
            c.drawString(100, 750, f"Risque d'abandon: {risk:.2%}")
            
            # Position initiale pour les d√©tails
            y = 720
            
            # √âcriture de chaque param√®tre d'entr√©e
            for k, v in input_data.items():
                c.drawString(100, y, f"{k}: {v}")
                y -= 20  # D√©placement vers le bas pour la ligne suivante
            
            # Sauvegarde du PDF
            c.save()
            
            # Retour au d√©but du buffer pour la lecture
            buffer.seek(0)
            
            # Bouton de t√©l√©chargement du rapport
            st.download_button("T√©l√©charger le rapport PDF", 
                             buffer, 
                             "rapport_abandon.pdf", 
                             "application/pdf")

# ============================================================================
# EX√âCUTION PRINCIPALE DE L'APPLICATION
# ============================================================================

# Titre principal de l'application
st.title("üéì Tableau de bord : Pr√©vention de l'abandon scolaire (OULAD)")

# Chargement des donn√©es OULAD
df_oulad = load_oulad_data()

# Lancement du pipeline d'analyse complet
analysis_pipeline(df_oulad, label='score_moyen', pdf_key="pdf_oulad")